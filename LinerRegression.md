# **概念**：

**机器学习**：机器学习是一类算法的总称，这些算法企图从大量历史数据中挖掘出其中隐含的规律，并用于预测或者分类，更具体的说，机器学习可以看作是寻找一个函数，输入是样本数据，输出是期望的结果，只是这个函数过于复杂，以至于不太方便形式化表达。

**有监督**：给机器训练数据拥有“标记”或者“答案”。比如：市场积累了房屋的基本信息和最终成交的金额，银行已经积累了一定的客户信息和他们信用卡的信用情况。

**无监督**：给机器训练数据没有任何“标记”或者“答案”。

**泛化能力**：需要注意的是，机器学习的目标是使学到的函数很好地适用于“新样本”，而不仅仅是在训练样本上表现很好。学到的函数适用于新样本的能力，称为泛化（Generalization）能力。

**过拟合**：一个模型在训练集上表现很好，在测试集上表现很差，这种情况发生了过拟合，权重参数浮动较大引起的。

**欠拟合**：是指模型拟合程度不高，数据距离拟合曲线较远，或指模型没有很好地捕捉到数据特征，不能够很好地拟合数据。解决办法：**增加新特征**，可以考虑加入进特征组合、高次特征，来增大假设空间；**添加多项式特征**，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强；**减少正则化参数**，正则化的目的是用来防止过拟合的，但是模型出现了欠拟合，则需要减少正则化参数；**使用非线性模型**，比如决策树、深度学习等模型；**调整模型的容量(capacity)**，通俗地，模型的容量是指其拟合各种函数的能力；

**交叉验证**：目的为了防范过拟合，我们把数据集分成训练集、验证集和测试集；训练集用来估算模型参数，验证集用来选择超参数，测试集用来评估模型效果。交叉验证是在机器学习建立模型和验证模型参数时常用的办法，顾名思义，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。



# 原理：

假设一个函数可以拟合所有的数据，求出这个函数的参数，并用该函数来进行预测。

代价/损失/目标函数：
$$
J(\theta_0,\theta_1,...\theta_n) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2
$$


梯度下降：
$$
\theta_j :=\theta_j - \alpha\frac{∂}{∂\theta_j}J(\theta_0,\theta_1,...\theta_n)\\=
\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^i)-y^i)x_j^i
$$


**牛顿法**：是为了求解函数值为零的时候变量的取值问题的。优点：牛顿法收敛速度相比梯度下降法很快，而且由于海森矩阵的的逆在迭代中不断减小，起到逐渐缩小步长的效果。缺点：牛顿法的缺点就是计算海森矩阵的逆比较困难，消耗时间和计算资源。因此有了拟牛顿法。

 **拟牛顿法**：牛顿法需要求海森矩阵，这个矩阵需要计算二阶偏导数，比较复杂。为了改良这个问题，提出了拟牛顿法。是通过：不求二阶偏导数，构造出一个近似的海森矩阵。

**线性回归评估指标**：

均方误差（MSE）、均方根误差（RMSE）、平均绝对误差 （MAE）、R^2^

**均方误差（MSE）:**

MSE （Mean Squared Error）叫做均方误差。用 真实值-预测值 然后平方之后求和平均。

```
y_preditc=reg.predict(x_test)                      #reg是训练好的模型
mse_test=np.sum((y_preditc-y_test)**2)/len(y_test) #跟数学公式一样的
```

**均方根误差（RMSE）:**

RMSE（Root Mean Squard Error）均方根误差。MSE开个根号。

```
rmse_test=mse_test ** 0.5
```

**MAE(平均绝对误差):**

```
mae_test=np.sum(np.absolute(y_preditc-y_test))/len(y_test)
```

**R^2**^:

组内变异（SSE）+组间变异（SSA）=总变异（SST），可以推出公式R^2^=1-SSE/SST

```
1- mean_squared_error(y_test,y_preditc)/ np.var(y_test)
```

参考链接：https://www.jianshu.com/p/9ee85fdad150

**Sklearn参数详解**：

```
from sklearn.linear_model import LinearRegression
LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)
'''
参数含义：
1.fit_intercept:布尔值，指定是否需要计算线性回归中的截距，即b值。如果为False,
那么不计算b值。
2.normalize:布尔值。如果为False，那么训练样本会进行归一化处理。
3.copy_X：布尔值。如果为True，会复制一份训练数据。
4.n_jobs:一个整数。任务并行时指定的CPU数量。如果取值为-1则使用所有可用的CPU。
返回值：
5.coef_:权重向量
6.intercept_:截距b值
 
方法：
1.fit(X,y)：训练模型。
2.predict(X)：用训练好的模型进行预测，并返回预测值。
3.score(X,y)：返回预测性能的得分。计算公式为：score=(1 - u/v)
其中u=((y_true - y_pred) ** 2).sum()，v=((y_true - y_true.mean()) ** 2).sum()
score最大值是1，但有可能是负值(预测效果太差)。score越大，预测性能越好。
'''
参考链接：https://blog.csdn.net/voidfaceless/article/details/61182436
```